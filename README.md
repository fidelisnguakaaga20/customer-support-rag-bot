
## ğŸ¥ Demo (Loom + YouTube)
ğŸ¥ ** code: https://github.com/fidelisnguakaaga20/customer-support-rag-bot
- ğŸ¥ **Loom:** https://www.loom.com/share/c9190d1c34054f3b84576e29ec832c67  
- ğŸ¥ **YouTube (Unlisted):** https://youtu.be/QnsbyYltVDo  

---

The demo shows:

1. Backend starting:  
   ```bash
   uvicorn fastapi_support_rag_api:app --reload --port 8000

2. Frontend running on `http://localhost:3000`
3. Asking questions like:
   - â€œWhat is your refund policy?â€
   - â€œHow long does shipping take?â€
   - â€œDo you ship internationally?â€
4. Short architecture walkthrough: **Next.js â†’ FastAPI â†’ FAISS â†’ local HF model**

---

## ğŸ§  What This Bot Does

- Loads a small **FAQ text file** with support policies.
- Splits it into chunks.
- Embeds chunks with a **local DistilBERT encoder**.
- Indexes them in **FAISS** (vector search).
- On each question:
  - Finds the most relevant FAQ chunks.
  - Builds a prompt with instructions + context.
  - Uses **distilgpt2** locally to generate an answer.
- Exposes a **FastAPI `/rag` endpoint**.
- Provides a simple **Next.js chat UI** to talk to the bot.

Everything is **CPU-only** and runs offline.

---

## ğŸ— Tech Stack

**Backend**

- Python 3.11+
- FastAPI
- Uvicorn
- HuggingFace Transformers
- sentence-transformers
- DistilBERT for embeddings
- DistilGPT-2 for generation
- FAISS (faiss-cpu) for vector search

**Frontend**

- Next.js
- TypeScript
- React
- Fetch-based client calling `POST /rag`

---

## ğŸ“‚ Project Structure

```text
customer-support-rag/
  backend/
    data/
      support_faq.txt
    fastapi_support_rag_api.py
    requirements.txt
    .venv/            # local, ignored by git

  frontend/
    app/
      page.tsx        # chat UI
    package.json
    tsconfig.json
    next.config.mjs
    ...

  .gitignore
  README.md           # this file
  file-tree.txt
  README.md roadmap   # high-level roadmap notes
````

---

## ğŸš€ How to Run (Local Only)

### 1. Backend (FastAPI + FAISS + local HF models)

```bash
cd backend
python -m venv .venv
source .venv/Scripts/activate    # on Windows Git Bash

pip install -r requirements.txt

uvicorn fastapi_support_rag_api:app --reload --port 8000
```

You should see logs like:

* `Loading embedding model: distilbert-base-uncased`
* `FAISS index built over X FAQ chunks`
* `Uvicorn running on http://127.0.0.1:8000`

### 2. Frontend (Next.js chat UI)

In another terminal:

```bash
cd frontend
npm install      # first time only
npm run dev
```

Open:

* `http://localhost:3000`

Ask:

* `What is your refund policy?`
* `How long does shipping take?`
* `Do you ship internationally?`

---

## ğŸ“ FAQ Data Source

The FAQ lives in:

```text
backend/data/support_faq.txt
```

Example content:

```text
Our refund policy: Customers can request a refund within 30 days of purchase.

Shipping usually takes between 3 and 5 business days.

If a product is damaged, we will send a replacement free of charge.

We do not ship internationally at this time.
```

To adapt this bot to another company, you only need to:

1. Edit `support_faq.txt`
2. Restart the backend

No code changes required.

---

## ğŸ”— How This Fits My LLM Roadmap

This project demonstrates:

* RAG over domain-specific text (support FAQ)
* Local embeddings + vector search (FAISS)
* Local generation (distilgpt2)
* FastAPI as LLM backend
* Next.js + TypeScript frontend integration

In my 3-month LLM engineering roadmap, this is:

* **Month 2 â€“ Week 5â€“8**: RAG + FastAPI + Next.js (Customer Support AI)
* **Month 3 â€“ Project 3** in the â€œ10 Portfolio Projectsâ€ list.

Other completed projects in the same roadmap:

1. Embedding Search Engine (Chroma + SentenceTransformers)
2. Resume RAG Chatbot (PDF resume + RAG + FastAPI + Next.js)
3. âœ… Customer Support RAG Bot (this project)

```
```
